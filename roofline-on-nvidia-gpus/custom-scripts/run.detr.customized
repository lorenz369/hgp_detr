#!/bin/bash -l
#SBATCH --constraint=gpu
#SBATCH --nodes=1
#SBATCH --gpus=1
#SBATCH --qos debug
#SBATCH --time=00:15:00
#SBATCH --account=m3930
#SBATCH --output=/global/homes/m/marcolz/DETR/gpu_reports/GPU1/slurm/slurm_%j.out

# --------------------------------------------------------------------------------
# This file has been modified from its original version. It retains the original
# copyright notice, terms, and disclaimer as specified in the Lawrence Berkeley 
# National Labs BSD variant license. The modifications are provided under the same 
# license terms as the original, with no endorsements from the original authors or 
# affiliated institutions unless expressly stated.
# --------------------------------------------------------------------------------

# pre-run
module load gpu
module load conda
conda activate detr_12.2
export MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)
export MASTER_PORT=12345
# dcgmi profile --pause



# detr parameters

export epochs=3
export backbone="resnet18"
export enc_layers=2
export dec_layers=2
export dim_ff=512
export hidden_dim=64
export nheads=4
export queries=20
export file="hgp"


input="python /global/homes/m/marcolz/DETR/hgp_detr/main.py"
input+=" --epochs $epochs --backbone $backbone --enc_layers $enc_layers --dec_layers $dec_layers --dim_feedforward $dim_ff --hidden_dim $hidden_dim --nheads $nheads --num_queries $queries --dataset_file $file"

dir=/global/homes/m/marcolz/DETR/hgp_detr/roofline-on-nvidia-gpus/custom-scripts

# Baseline
output=AMPscaledtimeoutput.txt
echo "$input > $dir/$output 2>&1"
$input  > $dir/$output 2>&1


# cd $dir
# srun -n1 python postprocess.py
